export DataSet, lnP, Î´lnP_Î´fÏ•â‚œ, HlnP, â„•, ğ•Š


# 
# This file contains function which compute things dealing with the posterior
# probability of f and Ï• given data, d. 
# 
# By definition, we take as our data model
# 
#     `d = M * B * L * f + n`
#
# where M, B, and L are the mask, beam/instrumental transfer functions, and
# lensing operators. Note this means that the noise n is defined as being
# unbeamed, and also is unmasked. If we're using simulated data, its easy to not
# mask the noise. For runs with real data, the noise outside the mask should be
# filled in with a realization of the noise. 
#
# Under this data model, the posterior probability is, 
# 
#     `-2 ln P(f,Ï•|d) = (d - M*B*L*fÌƒ)á´´*Cnâ»Â¹*(d - M*B*L*fÌƒ) + fá´´*Cfâ»Â¹*f + Ï•á´´*CÏ•â»Â¹*Ï•`
#
# The various covariances and M, B, and d are stored in a `DataSet` structure. 
#
# Below are also functions to compute derivatives of this likelihood, as well as
# a Wiener filter of the data (since that's `argmax_f P(f|Ï•,d)`).
#


# mixing matrix for mixed parametrization
D_mix(Cf::FullDiagOp; ÏƒÂ²len=deg2rad(5/60)^2) = @. nan2zero(sqrt((Cf+ÏƒÂ²len)/Cf))


# Stores variables needed to construct the likelihood
@with_kw struct DataSet{Td,TCn,TCf,TCfÌƒ,TCÏ•,TCnÌ‚,TBÌ‚,TM,TB,TD}
    d  :: Td                 # data
    Cn :: TCn                # noise covariance
    CÏ• :: TCÏ•                # Ï• covariance
    Cf :: TCf                # unlensed field covariance
    CfÌƒ :: TCfÌƒ  = nothing     # lensed field covariance (not always needed)
    CnÌ‚ :: TCnÌ‚  = Cn          # approximate noise covariance, diagonal in same basis as Cf
    BÌ‚  :: TBÌ‚   = B           # approximate beam and instrumental transfer functions, diagonal in same basis as Cf
    M  :: TM   = 1           # user mask
    B  :: TB   = 1           # beam and instrumental transfer functions
    D  :: TD   = D_mix(Cf)   # mixing matrix for mixed parametrization
end


## likelihood 


doc"""
    lnP(t, fâ‚œ, Ï•, ds, ::Type{L}=LenseFlow)
    lnP(t, fâ‚œ, Ï•, ds, L::LenseOp) 

Compute the log posterior probability as a function of the field, $f_t$, and the
lensing potential, $Ï•$. The subscript $t$ can refer to either a "time", e.g.
$t=0$ corresponds to the unlensed parametrization and $t=1$ to the lensed one,
or can be `:mix` correpsonding to the mixed parametrization. In all cases, the
argument `fâ‚œ` should then be $f$ in that particular parametrization.

The log posterior is defined such that, 

```math
-2 \ln \mathcal{P}(f,Ï•\,|\,d) = (d - \mathcal{M}\mathcal{B}\mathcal{L}{\tilde f})^{\dagger} \mathcal{C_n}^{-1} (d - \mathcal{M}\mathcal{B}\mathcal{L}{\tilde f}) \
                                + f^\dagger \mathcal{C_f}^{-1} f + \phi^\dagger \mathcal{C_\phi}^{-1} \mathcal{\phi}
```

The argument `ds` should be a `DataSet` and stores the masks, data, mixing
matrix, and covariances needed. `L` can be a type of lensing like `PowerLens` or
`LenseFlow`, or an already constructed `LenseOp`.
"""
lnP(t,fâ‚œ,Ï•,ds,::Type{L}=LenseFlow) where {L} = lnP(Val{t},fâ‚œ,Ï•,ds,L(Ï•))
lnP(t,fâ‚œ,Ï•,ds,L::LenseOp) = lnP(Val{t},fâ‚œ,Ï•,ds,L)

# log posterior in the unlensed or lensed parametrization
function lnP(::Type{Val{t}},fâ‚œ,Ï•,ds,L::LenseOp) where {t}
    @unpack Cn,Cf,CÏ•,M,B,d = ds
    Î” = d-M*B*L[tâ†’1]*fâ‚œ
    f = L[tâ†’0]*fâ‚œ
    -(Î”â‹…(Cn\Î”) + fâ‹…(Cf\f) + Ï•â‹…(CÏ•\Ï•))/2
end
# log posterior in the mixed parametrization
lnP(::Type{Val{:mix}},fÌ†,Ï•,ds,L::LenseOp) = (@unpack D = ds; lnP(0, D\(L\fÌ†), Ï•, ds, L))



## likelihood gradients

doc"""

    Î´lnP_Î´fÏ•â‚œ(t, fâ‚œ, Ï•, ds, ::Type{L}=LenseFlow)
    Î´lnP_Î´fÏ•â‚œ(t, fâ‚œ, Ï•, ds, L::LenseOp)

Compute a gradient of the log posterior probability. See `lnP` for definition of
arguments of this function. 

The return type is a `FieldTuple` corresponding to the $(f_t,\phi)$ derivative.
"""
Î´lnP_Î´fÏ•â‚œ(t,fâ‚œ,Ï•,ds,::Type{L}=LenseFlow) where {L} = Î´lnP_Î´fÏ•â‚œ(Val{t},fâ‚œ,Ï•,ds,L(Ï•))
Î´lnP_Î´fÏ•â‚œ(t,fâ‚œ,Ï•,ds,L::LenseOp) = Î´lnP_Î´fÏ•â‚œ(Val{t},fâ‚œ,Ï•,ds,L)

# derivatives of the three posterior probability terms at the times at which
# they're easy to take (used below)
Î´lnL_Î´fÌƒÏ•{Î¦}(fÌƒ,Ï•::Î¦,ds)  = (@unpack M,B,Cn,d=ds; FieldTuple(M'*B'*(Cn\(d-M*B*fÌƒ)), zero(Î¦)))
Î´lnÎ á¶ _Î´fÏ•{Î¦}(f,Ï•::Î¦,ds) = (@unpack Cf=ds;       FieldTuple(-Cf\f               , zero(Î¦)))
Î´lnÎ á¶²_Î´fÏ•{F}(f::F,Ï•,ds) = (@unpack CÏ•=ds;       FieldTuple(zero(F)             , -CÏ•\Ï•))


# log posterior gradient in the lensed or unlensed parametrization
function Î´lnP_Î´fÏ•â‚œ(::Type{Val{t}},fâ‚œ,Ï•,ds,L::LenseOp) where {t}
    fÌƒ =  L[tâ†’1]*fâ‚œ
    f =  L[tâ†’0]*fâ‚œ

    (    Î´lnL_Î´fÌƒÏ•(fÌƒ,Ï•,ds) * Î´fÌƒÏ•_Î´fÏ•â‚œ(L,fÌƒ,fâ‚œ,Val{t})
      + Î´lnÎ á¶ _Î´fÏ•(f,Ï•,ds) * Î´fÏ•_Î´fÏ•â‚œ(L,f,fâ‚œ,Val{t})
      + Î´lnÎ á¶²_Î´fÏ•(f,Ï•,ds))
end
# log posterior gradient in the mixed parametrization
function Î´lnP_Î´fÏ•â‚œ(::Type{Val{:mix}},fÌ†,Ï•,ds,L::LenseOp)

    @unpack D = ds
    Lâ»Â¹fÌ† = L \ fÌ†
    f = D \ Lâ»Â¹fÌ†

    # gradient w.r.t. (f,Ï•)
    Î´lnP_Î´f, Î´lnP_Î´Ï• = Î´lnP_Î´fÏ•â‚œ(0, f, Ï•, ds, L)
    
    # chain rule
    FieldTuple(Î´lnP_Î´f * D^-1, Î´lnP_Î´Ï•) * Î´fÏ•_Î´fÌƒÏ•(L, Lâ»Â¹fÌ†, fÌ†)
end




## wiener filter


doc"""
    lensing_wiener_filter(ds::DataSet, L, which=:wf)

Computes either, 
* the Wiener filter at fixed $\phi$, i.e. the best-fit of
$\mathcal{P}(f\,|\,\phi,d)$
* a sample from $\mathcal{P}(f\,|\,\phi,d)$

The data model assumed is, 

```math
d = \mathcal{M} \mathcal{B} \mathcal{L} \, f + n
```

Note that the noise is defined as un-debeamed and also unmasked (so it needs to
be filled in outside the mask if using real data). The mask, $\mathcal{M}$, can
be any composition of real and/or fourier space diagonal operators.
    
The argument `ds::DataSet` stores the mask, $\mathcal{M}$, the beam/instrumental
transfer functions, $\mathcal{B}$, as well as the various covariances which are
needed.

The `which` parameter controls which operation to do and can be one of three
things:

* `:wf` - Compute the Wiener filter
* `:sample` - Compute a sample from the posterior
* `:fluctuation` - Compute a fluctuation around the mean (i.e. a sample minus the Wiener filter)

"""
function lensing_wiener_filter(ds::DataSet{F}, L, which=:wf; guess=nothing, kwargs...) where F
    
    @unpack d, Cn, CnÌ‚, Cf, M, B, BÌ‚ = ds
    
    b = 0
    if (which in (:wf, :sample))
        b += L'*B'*M'*(Cn^-1)*d
    end
    if (which in (:fluctuation, :sample))
        b += sqrtm(Cf)\white_noise(F) + L'*B'*M'*(sqrtm(Cn)\white_noise(F))
    end
    
    pcg2(
        (Cf^-1) + BÌ‚'*(CnÌ‚^-1)*BÌ‚,
        (Cf^-1) + L'*B'*M'*(Cn^-1)*M*B*L,
        b,
        guess==nothing ? 0d : guess;
        kwargs...
    )
    
end


doc"""

    max_lnP_joint(ds::DataSet, L::Type{<:LenseOp}, nsteps=5, NÏ•=nothing, Ncg=500, cgtol=1e-1, Î±tol=1e-5, Î±max=1, progress=false)

Compute the maximum of the joint posterior, or a quasi-sample from the joint posterior. 

The `ds` argument stores the data and other relevant objects for the dataset
being considered. `L` gives which type of lensing operator to use. 

`NÏ•` can optionally specify an estimate of the Ï• effective noise, and if
provided is used to estimate a Hessian which is used in the Ï•
quasi-Newton-Rhapson step. `NÏ•=:qe` automatically uses the quadratic estimator
noise. 

`quasi_sample` can be set to an integer seed to compute a quasi-sample from the
posterior rather than the maximum. 

The following arguments control the maximiation procedure, and can generally be
left at their defaults:

* `nsteps` - The number of iteration steps to do (each iteration updates f then updates Ï•)
* `Ncg` - Maximum number of conjugate gradient steps during the f update
* `cgtol` - Conjugrate gradient tolerance (will stop at cgtol or Ncg, whichever is first)
* `Î±tol` - Tolerance for the linesearch in the Ï• quasi-Newton-Rhapson step, `xâ€² = x - Î±*Hâ»Â¹*g`
* `Î±max` - Maximum value for Î± in the linesearch
* `progress` - Whether to print out conjugate gradient progress.

"""
function max_lnP_joint(
    ds;
    L = LenseFlow,
    nsteps = 10, 
    NÏ• = nothing,
    Ncg = 500,
    cgtol = 1e-1,
    Î±tol = 1e-5,
    Î±max = 1.,
    quasi_sample = nothing, 
    progress = false)
    
    @unpack d, D, CÏ•, Cf, CfÌƒ, Cn = ds
    
    fcur, fÌŠcur = nothing, nothing
    Ï•cur = zero(Å(d)'Å(d)) # fix needing to get zero(Î¦) this way
    tr = []
    hist = nothing
    
    # compute approximate inverse Ï• Hessian used in gradient descent, possibly
    # from quadratic estimate
    if (NÏ• == :qe); NÏ• = Ï•qe(d, Cf, CfÌƒ, Cn)[2]; end
    HÏ•â»Â¹ = (NÏ• == nothing) ? CÏ• : (CÏ•^-1 + NÏ•^-1)^-1
    
    
    for i=1:nsteps

        # f step
        let L = (i==1 ? IdentityOp : cache(L(Ï•cur)))
            
            # if we're doing a quasi_sample, set the random seed here, which controls the
            # sample from the posterior we get from inside `lensing_wiener_filter`
            if (quasi_sample != nothing); srand(quasi_sample); end
            
            fcur,hist = lensing_wiener_filter(ds, L, 
                (quasi_sample==nothing) ? :wf : :sample, # if doing a quasi-sample, we get a sample instead of the WF
                guess=(i==1 ? nothing : fcur),           # after first iteration, use the previous f as starting point
                tol=cgtol, nsteps=Ncg, hist=(:i,:res), progress=progress)
                
            fÌŠcur = L * D * fcur
        end
        
        # Ï• step
        if i!=nsteps
            Ï•new = HÏ•â»Â¹*(Î´lnP_Î´fÏ•â‚œ(:mix,fÌŠcur,Ï•cur,ds,L))[2]
            res = optimize(Î±->(-lnP(:mix,fÌŠcur,Ï•cur+Î±*Ï•new,ds,L)), 0., Î±max, abs_tol=Î±tol)
            Î± = res.minimizer
            Ï•cur = Ï•cur+Î±*Ï•new
            lnPcur = -res.minimum
            if progress; @show i,lnPcur,length(hist),Î±; end
            push!(tr,@dictpack(i,lnPcur,hist,Î±,Ï•new,Ï•cur,fcur))
        end

    end

    return fÌŠcur, fcur, Ï•cur, tr
    
end



doc"""
    load_sim_dataset
    
Create a `DataSet` object with some simulated data. 

"""
function load_sim_dataset(;
    Î¸pix = throw(UndefVarError(:Î¸pix)),
    Nside = throw(UndefVarError(:Nside)),
    use = throw(UndefVarError(:use)),
    T = Float32,
    Î¼KarcminT = 3,
    â„“knee = 100,
    â„“max_data = 3000,
    beamFWHM = 0,
    Câ„“f = throw(UndefVarError(:Câ„“f)),
    Câ„“fÌƒ = throw(UndefVarError(:Câ„“fÌƒ)),
    seed = nothing,
    M = nothing,
    B = nothing,
    D = nothing,
    mask_kwargs = nothing,
    L = LenseFlow
    )
    
    # Câ„“s
    Câ„“n = noisecls(Î¼KarcminT, beamFWHM=0, â„“knee=â„“knee)
    
    # types which depend on whether T/E/B
    SS,ks = Dict(:TEB=>((S0,S2),(:TT,:EE,:BB,:TE)), :EB=>((S2,),(:EE,:BB)), :T=>((S0,),(:TT,)))[use]
    F,FÌ‚,nF = Dict(:TEB=>(FlatIQUMap,FlatTEBFourier,3), :EB=>(FlatS2QUMap,FlatS2EBFourier,2), :T=>(FlatS0Map,FlatS0Fourier,1))[use]
    
    # covariances
    P = Flat{Î¸pix,Nside}
    CÏ• = Câ„“_to_cov(T,P,S0, Câ„“f[:â„“], Câ„“f[:Ï•Ï•])
    Cf,CfÌƒ,Cn = (Câ„“_to_cov(T,P,SS..., Câ„“x[:â„“], (Câ„“x[k] for k=ks)...) for Câ„“x in (Câ„“f,Câ„“fÌƒ,Câ„“n))
    
    # data mask
    if (M == nothing) && (mask_kwargs != nothing)
        M = FullDiagOp(F{T,P}(repeated(T.(sptlike_mask(Nside,Î¸pix; mask_kwargs...)),nF)...)) * LP(â„“max_data)
    else
        M = LP(â„“max_data)
    end
    
    # beam
    if (B == nothing)
        B = let â„“=0:10000; Câ„“_to_cov(T,P,SS..., â„“, ((k==:TE ? 0.*â„“ : @.(exp(-â„“^2*deg2rad(beamFWHM/60)^2/(8*log(2))/2))) for k=ks)...); end;
    end
    
    # mixing matrix
    if (D == nothing); D = D_mix(Cf); end
    
    # simulate data
    if (seed != nothing); srand(seed); end
    f = simulate(Cf)
    Ï• = simulate(CÏ•)
    fÌƒ = L(Ï•)*f
    n = simulate(Cn)
    d = M*B*fÌƒ + n
    
    # put everything in DataSet
    ds = DataSet(;(@dictpack d Cn Cf CfÌƒ CÏ• M B D)...)
    
    return @dictpack f fÌƒ Ï• n ds T P
    
end
