export DataSet, lnP, Î´lnP_Î´fÏ•â‚œ, HlnP, â„•, ð•Š


# 
# This file contains function which compute things dealing with the posterior
# probability of f and Ï• given data, d. 
# 
# By definition, we take as our data model
# 
#     `d = P * M * B * L * f + n`
#
# where M, B, and L are the mask, beam/instrumental transfer functions, and
# lensing operators, and P is a pixelization operator. Since we track P, 
# it means we can estimate the fields on a higher resolution than the data. 
# Note also that this form means that the noise n is defined as being
# unbeamed, and also is unmasked. If we're using simulated data, its easy to not
# mask the noise. For runs with real data, the noise outside the mask should be
# filled in with a realization of the noise. 
#
# Under this data model, the posterior probability is, 
# 
#     `-2 ln P(f,Ï•|d) = (d - P*M*B*L*fÌƒ)á´´*Cnâ»Â¹*(d - P*M*B*L*fÌƒ) + fá´´*Cfâ»Â¹*f + Ï•á´´*CÏ•â»Â¹*Ï•`
#
# The various covariances and M, B, and d are stored in a `DataSet` structure. 
#
# Below are also functions to compute derivatives of this likelihood, as well as
# a Wiener filter of the data (since that's `argmax_f P(f|Ï•,d)`).
#


# mixing matrix for mixed parametrization
D_mix(Cf::LinOp; rfid=0.1, ÏƒÂ²len=deg2rad(5/60)^2) =
     ParamDependentOp((;r=rfid, _...)->(nan2zero.(sqrt.((Diagonal(evaluate(Cf,r=rfid))+ÏƒÂ²len) ./ Diagonal(evaluate(Cf,r=r))))))

# Stores variables needed to construct the likelihood
@with_kw struct DataSet{Td,TCn,TCf,TCfÌƒ,TCÏ•,TCnÌ‚,TBÌ‚,TM,TB,TD,TG,TP}
    d  :: Td                # data
    Cn :: TCn               # noise covariance
    CÏ• :: TCÏ•               # Ï• covariance
    Cf :: TCf               # unlensed field covariance
    CfÌƒ :: TCfÌƒ = nothing     # lensed field covariance (not always needed)
    CnÌ‚ :: TCnÌ‚ = Cn          # approximate noise covariance, diagonal in same basis as Cf
    M  :: TM  = 1           # user mask
    B  :: TB  = 1           # beam and instrumental transfer functions
    BÌ‚  :: TBÌ‚  = B           # approximate beam and instrumental transfer functions, diagonal in same basis as Cf
    D  :: TD  = IdentityOp  # mixing matrix for mixed parametrization
    G  :: TG  = IdentityOp  # reparametrization for Ï•
    P  :: TP  = 1           # pixelization operator to estimate field on higher res than data
end

function DataSet(ds::DataSet; kwargs...)
    FN = fieldnames(typeof(ds))
    DataSet(;NamedTuple{FN}(getfield.(Ref(ds),FN))..., kwargs...)
end

function (ds::DataSet)(;Î¸...)
    @unpack d,Cn,CÏ•,Cf,CfÌƒ,CnÌ‚,M,B,BÌ‚,D,G,P=ds
    DataSet(;@ntpack(d,M,B,BÌ‚,P,
        D=>evaluate(D;Î¸...),
        G=>evaluate(G;Î¸...),
        Cn=>evaluate(Cn;Î¸...),
        CÏ•=>evaluate(CÏ•;Î¸...),
        Cf=>evaluate(Cf;Î¸...),
        CfÌƒ=>evaluate(CfÌƒ;Î¸...),
        CnÌ‚=>evaluate(CnÌ‚;Î¸...))...)
end

    
@doc doc"""
    resimulate(ds::DataSet; f=..., Ï•=...)
    
Resimulate the data in a given dataset, potentially at a fixed f and/or Ï• (both
are resimulate if not provided)
"""
function resimulate(ds::DataSet; f=simulate(ds.Cf), Ï•=simulate(ds.CÏ•), n=simulate(ds.Cn), fÌƒ=LenseFlow(Ï•)*f)
    @unpack M,P,B = ds
    DataSet(ds, d = M*P*B*fÌƒ + n)
end


## likelihood 


@doc doc"""
    lnP(t, fâ‚œ, Ï•, ds, ::Type{L}=LenseFlow; Î¸...)
    lnP(t, fâ‚œ, Ï•, ds, L::LenseOp; Î¸...) 

Compute the log posterior probability as a function of the field, $f_t$, the
lensing potential, $\phi$, and possibly some cosmological parameters, $\theta$.
The subscript $t$ can refer to either a "time", e.g. passing `t=0` corresponds
to the unlensed parametrization and `t=1` to the lensed one, or can be `:mix`
correpsonding to the mixed parametrization. In all cases, the argument `fâ‚œ`
should then be $f$ in that particular parametrization.

The log posterior is defined such that, 

```math
-2 \ln \mathcal{P}(f,Ï•\,|\,d) = (d - \mathcal{M}\mathcal{B}\mathcal{L}{\tilde f})^{\dagger} \mathcal{C_n}(\theta)^{-1} (d - \mathcal{M}\mathcal{B}\mathcal{L}{\tilde f}) \
                                + f^\dagger \mathcal{C_f}(\theta)^{-1} f + \phi^\dagger \mathcal{C_\phi}(\theta)^{-1} \mathcal{\phi}
```

If any parameters $\theta$ are passed, we also include the three determinant
terms to properly normalize the posterior,

```math
+ \log\det\mathcal{C}_n(\theta) + \log\det\mathcal{C}_f(\theta) + \log\det\mathcal{C}_Ï•(\theta)
```

The argument `ds` should be a `DataSet` and stores the masks, data, mixing
matrix, and covariances needed. `L` can be a type of lensing like `PowerLens` or
`LenseFlow`, or an already constructed `LenseOp`.
"""
# this is the `lnP` method users will most likely call directly. first we switch t to Val(t)
lnP(t, fâ‚œ, Ï•, ds, L=LenseFlow; Î¸...) = lnP(Val(t), fâ‚œ, Ï•, ds, L; Î¸...)
# then evaluate L(Ï•) unless L was passed in already evaluated 
# (todo: remove repeated evaluation of ds(;Î¸...) which happens in the mixed case)
lnP(::Val{t},    fâ‚œ, Ï•,  ds, ::Type{L}; Î¸...) where {L<:LenseOp,t} = lnP(Val(t),    fâ‚œ, Ï•,  ds, cache(L(Ï•),fâ‚œ); Î¸...)
lnP(::Val{:mix}, fâ‚˜, Ï•â‚˜, ds, ::Type{L}; Î¸...) where {L<:LenseOp}   = lnP(Val(:mix), fâ‚˜, Ï•â‚˜, ds, cache(L(ds(;Î¸...).G\Ï•â‚˜),fâ‚˜); Î¸...)
# then evaluate ds at parameters Î¸, and undo the mixing if there was any
lnP(::Val{t}, fâ‚œ, Ï•, ds, L::LenseOp; Î¸...) where {t} = lnP(Val(t), fâ‚œ, Ï•, ds, ds(;Î¸...), L; Î¸...)
function lnP(::Val{:mix}, fâ‚˜, Ï•â‚˜, ds, L::LenseOp; Î¸...)
    dsÎ¸ = ds(;Î¸...)
    @unpack D,G = dsÎ¸
    (lnP(Val(0), D\(L\fâ‚˜), G\Ï•â‚˜, ds, dsÎ¸, L; Î¸...)
     - (depends_on(ds.D, Î¸) ? logdet(D) : 0)
     - (depends_on(ds.G, Î¸) ? logdet(G) : 0))
end
# finally, evaluate the actual posterior
function lnP(::Val{t}, fâ‚œ, Ï•, ds::DataSet, dsÎ¸::DataSet, L::LenseOp; Î¸...) where {t}
    
    # unpack needed variables from the dataset evaluated at Î¸
    @unpack Cn,Cf,CÏ•,M,P,B,d = dsÎ¸
    
    # the unnormalized part of the posterior
    Î” = d-M*P*B*L[tâ†’1]*fâ‚œ
    f = L[tâ†’0]*fâ‚œ
    lnP = -(Î”â‹…(Cn\Î”) + fâ‹…(Cf\f) + Ï•â‹…(CÏ•\Ï•))/2
    
    # add the normalization (the logdet terms), offset by its value at fiducial
    # parameters (to avoid roundoff errors, since its otherwise a large number).
    # note: only the terms which depend on parameters that were passed in via
    # `Î¸... ` will be computed. 
    lnP += (lnP_logdet_terms(ds,dsÎ¸; Î¸...) - lnP_logdet_terms(ds,ds(); Î¸...))

    lnP
    
end

# logdet terms in the posterior given the covariances in `dsÎ¸` which is the
# dataset evaluated at parameters Î¸.  `ds` is used to check which covariances
# were param-dependent prior to evaluation, and these are not calculated
function lnP_logdet_terms(ds, dsÎ¸; Î¸...)
    -(  (depends_on(ds.Cn, Î¸) ? logdet(dsÎ¸.Cn) : 0) 
      + (depends_on(ds.Cf, Î¸) ? logdet(dsÎ¸.Cf) : 0)
      + (depends_on(ds.CÏ•, Î¸) ? logdet(dsÎ¸.CÏ•) : 0))/2
end



## joint posterior gradients

@doc doc"""

    Î´lnP_Î´fÏ•â‚œ(t, fâ‚œ, Ï•, ds, ::Type{L}=LenseFlow)
    Î´lnP_Î´fÏ•â‚œ(t, fâ‚œ, Ï•, ds, L::LenseOp)

Compute a gradient of the log posterior probability. See `lnP` for definition of
arguments of this function. 

The return type is a `FieldTuple` corresponding to the $(f_t,\phi)$ derivative.
"""
# this is the `Î´lnP_Î´fÏ•â‚œ` method users will most likely call directly. first we
# switch t to Val(t) and evaluate at parameters Î¸
Î´lnP_Î´fÏ•â‚œ(t, fâ‚œ, Ï•, ds, L=LenseFlow; Î¸...) = Î´lnP_Î´fÏ•â‚œ(Val(t), fâ‚œ, Ï•, ds(;Î¸...), L)
# in the lensed or unlensed parametrization
Î´lnP_Î´fÏ•â‚œ(::Val{t}, fâ‚œ, Ï•, ds, ::Type{L}) where {L<:LenseOp,t} = Î´lnP_Î´fÏ•â‚œ(Val(t), fâ‚œ, Ï•, ds, cache(L(Ï•),fâ‚œ))
Î´lnP_Î´fÏ•â‚œ(::Val{t}, fâ‚œ, Ï•, ds, L::LenseOp) where {t} = begin
    fÌƒ =  L[tâ†’1]*fâ‚œ
    f =  L[tâ†’0]*fâ‚œ

    (   Î´fÌƒÏ•_Î´fÏ•â‚œ(L,fÌƒ,fâ‚œ,Val(t))' * Î´lnL_Î´fÌƒÏ•(fÌƒ,Ï•,ds)
      + Î´fÏ•_Î´fÏ•â‚œ(L,f,fâ‚œ,Val(t))' * Î´lnÎ á¶ _Î´fÏ•(f,Ï•,ds)
                                 + Î´lnÎ á¶²_Î´fÏ•(f,Ï•,ds) )
end
# in the mixed parametrization
Î´lnP_Î´fÏ•â‚œ(::Val{:mix}, fâ‚˜, Ï•â‚˜, ds, L::LenseOp) = Î´lnP_Î´fÏ•â‚œ(Val(:mix), fâ‚˜, Ï•â‚˜, ds.G\Ï•â‚˜, ds, L)
Î´lnP_Î´fÏ•â‚œ(::Val{:mix}, fâ‚˜, Ï•â‚˜, ds, ::Type{L}) where {L<:LenseOp} = begin
    Ï• = ds.G\Ï•â‚˜
    Î´lnP_Î´fÏ•â‚œ(Val(:mix), fâ‚˜, Ï•â‚˜, Ï•, ds, cache(L(Ï•),fâ‚˜))
end
Î´lnP_Î´fÏ•â‚œ(::Val{:mix}, fâ‚˜, Ï•â‚˜, Ï•, ds, L::LenseOp) = begin
    
    @unpack D,G = ds
    Lâ»Â¹fâ‚˜ = L \ fâ‚˜
    f = D \ Lâ»Â¹fâ‚˜

    # gradient w.r.t. (f,Ï•)
    Î´lnP_Î´f, Î´lnP_Î´Ï• = Î´lnP_Î´fÏ•â‚œ(0, f, Ï•, ds, L)
    
    # chain rule
    (Î´lnP_Î´fâ‚˜, Î´lnP_Î´Ï•â‚˜) = Î´fÏ•_Î´fÌƒÏ•(L, Lâ»Â¹fâ‚˜, fâ‚˜)' * FieldTuple(D \ Î´lnP_Î´f, Î´lnP_Î´Ï•)
    FieldTuple(Î´lnP_Î´fâ‚˜, G \ Î´lnP_Î´Ï•â‚˜)

end
# derivatives of the three posterior probability terms at the times at which
# they're easy to take (used above)
Î´lnL_Î´fÌƒÏ•(fÌƒ,Ï•::É¸,ds)  where {É¸} = (@unpack P,M,B,Cn,d=ds; FieldTuple(B'*P'*M'*(Cn\(d-M*P*B*fÌƒ)), zero(É¸)))
Î´lnÎ á¶ _Î´fÏ•(f,Ï•::É¸,ds) where {É¸} = (@unpack Cf=ds;         FieldTuple(-(Cf\f)                  , zero(É¸)))
Î´lnÎ á¶²_Î´fÏ•(f::F,Ï•,ds) where {F} = (@unpack CÏ•=ds;         FieldTuple(zero(F)                  , -(CÏ•\Ï•)))



## marginal posterior gradients

Î´lnP_Î´Ï•(Ï•, ds, ::Type{L}=LenseFlow; kwargs...) where {L} = Î´lnP_Î´Ï•(L(Ï•), ds; kwargs...)

function Î´lnP_Î´Ï•(L::LenseOp, ds; Nmc_det=100, progress=false, return_sims=false)
    
    @unpack d,P,M,B,Cn,Cf,CnÌ‚,G = ds
    
    if G!=IdentityOp; @warn "Î´lnP_Î´Ï• does not currently handle the G mixing matrix"; end

    function gQD(L, ds)
        y = B' * M' * P' * (Î£(L, ds) \ ds.d)
        y * Î´Lf_Î´Ï•(Cf*(L'*y), L)
    end

    det_sims = @showprogress pmap(1:Nmc_det) do i gQD(L, resimulate(ds, fÌƒ=L*simulate(ds.Cf))) end

    g = gQD(L, ds) - mean(det_sims)

    return_sims ? (g, det_sims) : g 

end




## wiener filter


@doc doc"""
    lensing_wiener_filter(ds::DataSet, L, which=:wf)

Computes either, 
* the Wiener filter at fixed $\phi$, i.e. the best-fit of
$\mathcal{P}(f\,|\,\phi,d)$
* a sample from $\mathcal{P}(f\,|\,\phi,d)$

The data model assumed is, 

```math
d = \mathcal{M} \mathcal{B} \mathcal{L} \, f + n
```

Note that the noise is defined as un-debeamed and also unmasked (so it needs to
be filled in outside the mask if using real data). The mask, $\mathcal{M}$, can
be any composition of real and/or fourier space diagonal operators.
    
The argument `ds::DataSet` stores the mask, $\mathcal{M}$, the beam/instrumental
transfer functions, $\mathcal{B}$, as well as the various covariances which are
needed.

The `which` parameter controls which operation to do and can be one of three
things:

* `:wf` - Compute the Wiener filter
* `:sample` - Compute a sample from the posterior
* `:fluctuation` - Compute a fluctuation around the mean (i.e. a sample minus the Wiener filter)

"""
function lensing_wiener_filter(ds::DataSet{F}, L, which=:wf; guess=nothing, kwargs...) where F
    
    @unpack d, Cn, CnÌ‚, Cf, M, B, P, BÌ‚ = ds
    
    b = 0
    if (which in (:wf, :sample))
        b += L'*B'*P'*M'*(Cn^-1)*d
    end
    if (which in (:fluctuation, :sample))
        b += Cf\simulate(Cf) + L'*B'*P'*M'*(Cn\simulate(Cn))
    end
    
    pcg2(
        (Cf^-1) + BÌ‚'*(CnÌ‚^-1)*BÌ‚,
        (Cf^-1) + L'*B'*P'*M'*(Cn^-1)*M*P*B*L,
        b,
        guess==nothing ? 0*b : guess;
        kwargs...
    )
    
end


# todo: figure out if this and `lensing_wiener_filter` above are the same and
# can be combined
@doc doc"""
    Î£(Ï•, ds, ::Type{L}=LenseFlow) where {L}
    Î£(L::LenseOp, ds) 
    
Operator for the data covariance, Cn + P*M*B*L*Cf*L'*B'*M'*P', which can applied
and inverted.
"""
Î£(Ï•, ds, ::Type{L}=LenseFlow) where {L} = Î£(L(Ï•),ds)
Î£(L::LenseOp, ds) = begin

    @unpack d,P,M,B,Cn,Cf,CnÌ‚, BÌ‚ = ds

    SymmetricFuncOp(
        op   = x -> (Cn + P*M*B*L*Cf*L'*B'*M'*P')*x,
        opâ»Â¹ = x -> pcg2((CnÌ‚ .+ BÌ‚*Cf*BÌ‚'), Î£(L, ds), x, nsteps=100, tol=1e-1)
    )

end



@doc doc"""

    MAP_joint(ds::DataSet; L=LenseFlow, NÏ•=nothing, quasi_sample=nothing, nsteps=10, Ncg=500, cgtol=1e-1, Î±tol=1e-5, Î±max=0.5, progress=false)

Compute the maximum a posteri estimate (MAP) from the joint posterior (can also
do a quasi-sample). 

The `ds` argument stores the data and other relevant objects for the dataset
being considered. `L` gives which type of lensing operator to use. 

`Ï•start` can be used to specify the starting point of the minimizer, but this is
not necessary and otherwise it will start at Ï•=0. 

`NÏ•` can optionally specify an estimate of the Ï• effective noise, and if
provided is used to estimate a Hessian which is used in the Ï•
quasi-Newton-Rhapson step. `NÏ•=:qe` automatically uses the quadratic estimator
noise. 

This function can also be used to draw quasi-samples, wherein for the f step, we
draw a sample from  P(f|Ï•) instead of maximizing it (ie instead of computing
Wiener filter). `quasi_sample` can be set to an integer seed, in which case each
time in the `f` step we draw a same-seeded sample. If `quasi_sample` is instead
just `true`, then each iteration in the algorithm draws a different sample so
the solution bounces around rather than asymptoting to a maximum. 

The following arguments control the maximiation procedure, and can generally be
left at their defaults:

* `nsteps` - The number of iteration steps to do (each iteration updates f then updates Ï•)
* `Ncg` - Maximum number of conjugate gradient steps during the f update
* `cgtol` - Conjugrate gradient tolerance (will stop at cgtol or Ncg, whichever is first)
* `Î±tol` - Tolerance for the linesearch in the Ï• quasi-Newton-Rhapson step, `xâ€² = x - Î±*Hâ»Â¹*g`
* `Î±max` - Maximum value for Î± in the linesearch
* `progress` - Whether to print out conjugate gradient progress.

Returns a tuple `(f, Ï•, tr)` where `f` is the best-fit (or quasi-sample) field,
`Ï•` is the lensing potential, and `tr` contains info about the run. 

"""
function MAP_joint(
    ds;
    Ï•start = nothing,
    L = LenseFlow,
    NÏ• = nothing,
    quasi_sample = false, 
    nsteps = 10, 
    Ncg = 500,
    cgtol = 1e-1,
    Î±tol = 1e-5,
    Î±max = 0.5,
    cache_function = (L->cache(L,ds.d)),
    callback = nothing,
    progress = false)
    
    if !(isa(quasi_sample,Bool) || isa(quasi_sample,Int))
        throw(ArgumentError("quasi_sample should be true, false, or an Int."))
    end
    
    # since MAP estimate is done at fixed Î¸, we don't need to reparametrize to
    # Ï•â‚˜ = G(Î¸)*Ï•, so set G to constant here
    ds = DataSet(ds, G=IdentityOp)
    @unpack d, D, CÏ•, Cf, CfÌƒ, Cn, CnÌ‚ = ds
    
    fcur, fâ‚˜cur = nothing, nothing
    Ï•cur = (Ï•start != nothing) ? Ï•start : Ï•cur = zero(CÏ•)
    Î± = 0
    tr = []
    hist = nothing
    
    # compute approximate inverse Ï• Hessian used in gradient descent, possibly
    # from quadratic estimate
    if (NÏ• == :qe); NÏ• = Ï•qe(ds,false)[2]/2; end
    HÏ•â»Â¹ = (NÏ• == nothing) ? CÏ• : (CÏ•^-1 + NÏ•^-1)^-1
    
    try
        for i=1:nsteps

            # f step
            let L = ((i==1 && Ï•start==nothing) ? IdentityOp : cache_function(L(Ï•cur)))
                
                # if we're doing a fixed quasi_sample, set the random seed here,
                # which controls the sample from the posterior we get from inside
                # `lensing_wiener_filter`
                if isa(quasi_sample,Int); seed!(quasi_sample); end
                
                fcur,hist = lensing_wiener_filter(ds, L, 
                    (quasi_sample==false) ? :wf : :sample,   # if doing a quasi-sample, we get a sample instead of the WF
                    guess=(i==1 ? nothing : fcur),           # after first iteration, use the previous f as starting point
                    tol=cgtol, nsteps=Ncg, hist=(:i,:res), progress=progress)
                    
                fâ‚˜cur = L * D * fcur
            end
            
            lnPcur = lnP(:mix,fâ‚˜cur,Ï•cur,ds,L)
            if progress
                @printf("(step=%i, Ï‡Â²=%.2f, Ncg=%i%s)\n", i, -2lnPcur, length(hist), (Î±==0 ? "" : @sprintf(", Î±=%.6f",Î±)))
            end
            push!(tr,@dictpack(i,lnPcur,hist,Ï•cur,fcur))
            if callback != nothing
                callback(fcur, Ï•cur, tr)
            end
            
            # Ï• step
            if i!=nsteps
                Ï•new = HÏ•â»Â¹*(Î´lnP_Î´fÏ•â‚œ(:mix,fâ‚˜cur,Ï•cur,ds,L))[2]
                res = optimize(Î±->(-lnP(:mix,fâ‚˜cur,Ï•cur+Î±*Ï•new,ds,L)), 0., Î±max, abs_tol=Î±tol)
                Î± = res.minimizer
                Ï•cur = Ï•cur+Î±*Ï•new
            end

        end
    catch err
        if err isa InterruptException
            println()
            @warn("Maximization interrupted. Returning current progress.")
        else
            rethrow(err)
        end
    end

    return fcur, Ï•cur, tr
    
end


@doc doc"""

    MAP_marg( ds; kwargs...)

Compute the maximum a posteri estimate (MAP) of the marginl posterior.
"""
function MAP_marg(
    ds;
    Ï•start = nothing,
    L = LenseFlow,
    NÏ• = nothing,
    nsteps = 10, 
    Ncg = 500,
    cgtol = 1e-1,
    Î± = 0.02,
    Nmc_det = 50,
    )
    
    @unpack Cf, CÏ•, CfÌƒ, CnÌ‚ = ds
    
    # compute approximate inverse Ï• Hessian used in gradient descent, possibly
    # from quadratic estimate
    if (NÏ• == :qe); NÏ• = Ï•qe(zero(Cf), Cf, CfÌƒ, CnÌ‚)[2]; end
    HÏ•â»Â¹ = (NÏ• == nothing) ? CÏ• : (CÏ•^-1 + NÏ•^-1)^-1

    Ï•cur = (Ï•start != nothing) ? Ï•start : Ï•cur = zero(CÏ•) # fix needing to get zero(É¸) this way
    tr = []

    for i=1:nsteps
        g, det_sims = Î´lnP_Î´Ï•(Ï•cur, ds, progress=true, Nmc_det=Nmc_det, return_sims=true)
        Ï•cur += Î± * HÏ•â»Â¹ * g
        push!(tr,@dictpack(i,g,det_sims,Ï•cur))
    end
    
    return Ï•cur, tr

end



@doc doc"""
    load_sim_dataset
    
Create a `DataSet` object with some simulated data. 

"""
function load_sim_dataset(;
    Î¸pix,
    Î¸pix_data = Î¸pix,
    Nside,
    use,
    T = Float32,
    Î¼KarcminT = 3,
    â„“knee = 100,
    Î±knee = 3,
    â„“max_data = 3000,
    beamFWHM = 0,
    rfid = 0.05,
    Câ„“ = camb(r=rfid),
    Câ„“n = nothing,
    Cn = nothing,
    seed = nothing,
    M = nothing,
    B = nothing,
    D = nothing,
    G = nothing,
    mask_kwargs = nothing,
    L = LenseFlow,
    âˆ‚mode = fourierâˆ‚
    )
    
    # the biggest â„“ on the 2D fourier grid
    â„“max = round(Int,ceil(âˆš2*FFTgrid(T,Flat{Î¸pix,Nside}).nyq))
    
    # Câ„“s
    if (Câ„“n == nothing)
        Câ„“n = noisecls(Î¼KarcminT, beamFWHM=0, â„“knee=â„“knee, Î±knee=Î±knee, â„“max=â„“max)
    end
    Câ„“f, Câ„“fÌƒ = Câ„“[:f], Câ„“[:fÌƒ]
    
    # types which depend on whether T/E/B
    if (use == :EB)
        @warn("switch to use=:P")
        use = :P
    elseif (use == :TEB)
        @warn("switch to use=:TP")
        use = :TP
    end
    SS,ks = Dict(:TP=>((S0,S2),(:TT,:EE,:BB,:TE)), :P=>((S2,),(:EE,:BB)), :T=>((S0,),(:TT,)))[use]
    F,FÌ‚,nF = Dict(:TP=>(FlatIQUMap,FlatTEBFourier,3), :P=>(FlatS2QUMap,FlatS2EBFourier,2), :T=>(FlatS0Map,FlatS0Fourier,1))[use]
    
    # pixelization
    P = (Î¸pix_data == Î¸pix) ? 1 : FuncOp(
        op  = f -> ud_grade(f, Î¸pix_data, deconv_pixwin=false, anti_aliasing=false),
        opá´´ = f -> ud_grade(f, Î¸pix,      deconv_pixwin=false, anti_aliasing=false)
    )
    Pix      = Flat{Î¸pix,Nside,âˆ‚mode}
    Pix_data = Flat{Î¸pix_data,NsideÃ·(Î¸pix_dataÃ·Î¸pix),âˆ‚mode}
    
    # covariances
    CÏ•â‚€               =  Câ„“_to_cov(T,Pix,     S0,    Câ„“f[:Ï•Ï•])
    Cfs,Cft,CfÌƒ,CnÌ‚     = (Câ„“_to_cov(T,Pix,     SS..., (Câ„“x[k] for k=ks)...) for Câ„“x in (Câ„“[:fs],Câ„“[:ft],Câ„“fÌƒ,Câ„“n))
    if (Cn == nothing)
        Cn            =  Câ„“_to_cov(T,Pix_data,SS..., (Câ„“n[k] for k=ks)...)
    end
    Cf = ParamDependentOp((;r=rfid, _...)->(@. Cfs + (r/rfid)*Cft))
    CÏ• = ParamDependentOp((;AÏ•=1,   _...)->(@. AÏ•*CÏ•â‚€))
    
    # data mask
    if (M == nothing) && (mask_kwargs != nothing)
        M = LowPass(â„“max_data) * FullDiagOp(F{T,Pix_data}(repeated(T.(sptlike_mask(NsideÃ·(Î¸pix_dataÃ·Î¸pix),Î¸pix_data; mask_kwargs...)),nF)...))
    elseif (M == nothing)
        M = LowPass(â„“max_data)
    end
    
    # beam
    if (B == nothing)
        B = let â„“=0:â„“max; Câ„“_to_cov(T,Pix,SS..., (InterpolatedCâ„“s(â„“, (k==:TE ? zero(â„“) : @.(exp(-â„“^2*deg2rad(beamFWHM/60)^2/(8*log(2))/2)))) for k=ks)...); end;
    end
    
    # mixing matrices
    if (D == nothing); D = D_mix(Cf); end
    if (G == nothing); G = IdentityOp; end
    
    # simulate data
    if (seed != nothing); seed!(seed); end
    Ï• = simulate(CÏ•)
    f = simulate(Cf)
    fÌƒ = L(Ï•)*f
    n = simulate(Cn)
    d = M*P*B*fÌƒ + n
    
    # put everything in DataSet
    ds = DataSet(;(@ntpack d Cn CnÌ‚ Cf CfÌƒ CÏ• M B D G P)...)
    
    return @ntpack f fÌƒ Ï• n ds dsâ‚€=>ds() T P=>Pix 
    
end

function Ï•qe(ds::DataSet, wiener_filtered=false)
    @unpack d, Cf, CfÌƒ, CnÌ‚, CÏ•, B = ds
    CfÌƒ = B^2 * CfÌƒ
    Cf = B^2 * Cf
    wiener_filtered ? Ï•qe(d, Cf, CfÌƒ, CnÌ‚, CÏ•) : Ï•qe(d, Cf, CfÌƒ, CnÌ‚)
end
